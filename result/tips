不知道什么原因，但是传统的KD最好按照下面的参数进行设置：

args = TrainingArguments(
    output_dir='./results',  # 输出目录
    num_train_epochs=20,  # 训练轮次

    do_train=True,  # 启用训练模式

    per_device_train_batch_size=4,  # 单设备批次大小
    gradient_accumulation_steps=16,  # 梯度累积步数

    logging_steps=500,  # 日志记录间隔

    save_strategy="epoch",
    save_total_limit=1,  # 最大保存检查点数
    bf16=True,  # 使用bfloat16精度
    learning_rate=0.0005,  # 学习率
    lr_scheduler_type='constant',  # 恒定学习率
    optim="adamw_torch_fused",  # 使用融合AdamW优化器
)

# 初始化知识蒸馏训练器
trainer = KDTrainer(
    model=student,  # 学生模型
    teacher_model=teacher,  # 教师模型

    if_use_entropy=True,  # 启用混合损失
    processing_class=tokenizer,  # 使用教师模型的tokenizer

    train_dataset=dataset,  # 训练数据集

    dataset_text_field="text",  # 文本字段名
    max_seq_length=max_seq_length,  # 最大序列长度
    dataset_num_proc=2,  # 数据集处理进程数
    packing=False,  # 禁用序列打包（短序列时可加速）
    args=args,  # 训练参数配置
)

EMD的参数还在探索：
