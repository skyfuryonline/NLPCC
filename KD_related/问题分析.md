### 2. 最优传输推理蒸馏（Optimal Transport for Reasoning Distillation）

**目标**：通过序列级OT对齐，解决不同分词器和序列长度的对齐问题，保留上下文语义。

### 关键步骤：

1. **经验分布定义**：
    - 学生和教师的序列分别表示为$\mathbf{x} \in \mathbb{R}^{N \times d}$ 和 $\mathbf{y} \in \mathbb{R}^{M \times D}$，其经验分布为：
    $\mu = \frac{1}{N}\sum_{i=1}^N \delta_{\mathbf{x}i}, \quad \nu = \frac{1}{M}\sum_{j=1}^M \delta_{\mathbf{y}_j}$
    - $N和M$ 分别为学生和教师序列的长度；（即学生序列中每个token获得相同的权重1/N，教师序列中每个token获得相同的权重1/M）
    - d 和 D 是每个 token 的维度；
    - δ(⋅) 用于表示在每个位置的 token；定义每个 token 在序列中的“位置”和分布;
2. **成本矩阵计算**：
    - **相似性矩阵**：通过投影教师嵌入到学生空间：
    $\mathbf{S} = \frac{\mathbf{X} \mathbf{P}(\mathbf{Y})^\top}{\sqrt{d}}$
    其中：
        - $\mathbf{P} \in \mathbb{R}^{D \times d}$为投影矩阵，将教师的嵌入映射到学生的空间，而缩放因子
        
        $\sqrt{d}$确保了数值稳定性
        
        - $X \in \mathbb{R}^{N \times d}$和$Y \in \mathbb{R}^{M \times D}$分别是学生和教师的 token 嵌入表示。
    - **归一化与成本矩阵**：相似度矩阵通过softmax函数按行(row-wise)归一化(这种操作确保了矩阵的每一行在经过归一化后都变成一个概率分布—-符合概率分布性质),确保每一行的和为1；其中，下面的1表示一个全为1的矩阵;
    $\mathbf{S}_{\text{norm}} = \text{softmax}(\mathbf{S}), \quad \mathbf{C} = 1 - \mathbf{S}_{\text{norm}}$
3. **最优传输计划求解**：
    - 熵正则化OT问题：
    - 最优传输问题的目标是寻找一个最优的传输计划$T^*$，以最小化从一个分布到另一个分布的成本
    $\mathbf{T}^* = \arg\min_{\mathbf{T} \in U(\alpha, \beta)} \langle \mathbf{T}, \mathbf{C} \rangle - \frac{1}{\lambda} H(\mathbf{T})$
        - T 是传输矩阵，$T_{ij}$表示从第i个样本到第j个样本的传输量；
        - $C_{ij}$是从第i到第j的传输成本；
        - H(T)是熵正则化项 :$H(T)=−\sum_{i,j}T_{ij}\log T_{ij}$
        - λ 是熵正则化项的超参数，控制熵项对优化的影响；
    - OT损失定义为：
    $\mathcal{L}_{OT}(\mathbf{x}, \mathbf{y}) = \langle \mathbf{T}^*, \mathbf{C} \rangle$
如上展示的是OT进行层级对齐的蒸馏损失介绍，请帮我根据上述介绍的原理，对Qwen-1.5B和Qwen-7B进行蒸馏设计，要求：
1.将蒸馏损失应用于模型最后的logits部分；
2.要求在单张4090上可以进行训练；
3.模型使用：Qwen2.5-1.5B-bnb-4bit和Qwen2.5-7B
4.考虑到logits的维度会导致代价矩阵巨大，请使用分块矩阵的思路进行分割，并分析具体的显存占用情况。

好的，我将用中文重新回答您的问题，并根据您的要求设计基于最优传输（Optimal Transport, OT）的蒸馏方案，将 Qwen2.5-1.5B-bnb-4bit（学生模型）蒸馏自 Qwen2.5-7B（教师模型），将 OT 损失应用于模型的最终 logits 部分，并在单张 NVIDIA 4090 GPU（24 GB 显存）上进行训练。以下是详细设计和显存分析。

---

### 设计概览

1. **目标**：使用 OT 在序列级别对齐学生模型（Qwen2.5-1.5B-bnb-4bit）和教师模型（Qwen2.5-7B）的 logits，保留上下文语义。
2. **应用位置**：将 OT 蒸馏损失应用于模型的最终 logits 层。
3. **硬件限制**：训练需在单张 4090 GPU（24 GB 显存）上运行。
4. **方法**：由于 logits 维度较大导致成本矩阵过大，采用分块矩阵（block matrix）方法分割计算，并分析显存占用。

---

### 第一步：模型设置

- **学生模型**：Qwen2.5-1.5B-bnb-4bit
  - 参数量：约 15 亿。
  - 量化：4-bit（基于 BitsAndBytes），显著降低显存需求。
  - 隐藏层维度（d）：1536（基于 Qwen2.5 架构）。
- **教师模型**：Qwen2.5-7B
  - 参数量：约 70 亿。
  - 默认精度：FP16（为简单起见，假设未量化）。
  - 隐藏层维度（D）：3584（基于 Qwen2.5 架构）。
- **Logits 维度**：词汇表大小 \( V = 151,936 \)（Qwen2.5 分词器），每个 token 的 logits 维度为 \( V \)。

---

### 第二步：基于 Logits 的 OT 蒸馏设计

#### 输入表示
- **学生 Logits**：\(\mathbf{X} \in \mathbb{R}^{N \times V}\)，其中 \( N \) 为序列长度。
- **教师 Logits**：\(\mathbf{Y} \in \mathbb{R}^{M \times V}\)，其中 \( M \) 为序列长度（为简化，假设 \( N = M \) 并设为 512，padding 或截断）。
- **经验分布**：
  - 学生：\(\mu = \frac{1}{N} \sum_{i=1}^N \delta_{\mathbf{x}_i}\)。
  - 教师：\(\nu = \frac{1}{M} \sum_{j=1}^M \delta_{\mathbf{y}_j}\)。

#### 相似性矩阵计算
- 由于学生和教师的 logits 维度相同（均为 \( V \)），无需额外的投影矩阵 \(\mathbf{P}\)，可直接计算：
  \[
  \mathbf{S} = \frac{\mathbf{X} \mathbf{Y}^\top}{\sqrt{V}}
  \]
  - \(\mathbf{S} \in \mathbb{R}^{N \times M}\) 表示相似性矩阵。
  - 缩放因子 \(\sqrt{V}\) 确保数值稳定性。
- **归一化**：按行（row-wise）对 \(\mathbf{S}\) 应用 softmax：
  \[
  \mathbf{S}_{\text{norm}} = \text{softmax}(\mathbf{S})
  \]
  - 每行和为 1，满足概率分布性质。
- **成本矩阵**：
  \[
  \mathbf{C} = 1 - \mathbf{S}_{\text{norm}}
  \]
  - \(\mathbf{C} \in \mathbb{R}^{N \times M}\)。

#### OT 损失计算
- **熵正则化 OT 问题**：
  \[
  \mathbf{T}^* = \arg\min_{\mathbf{T} \in U(\alpha, \beta)} \langle \mathbf{T}, \mathbf{C} \rangle - \frac{1}{\lambda} H(\mathbf{T})
  \]
  - \(\mathbf{T}\) 为传输矩阵，\( T_{ij} \) 表示从学生 token \( i \) 到教师 token \( j \) 的传输量。
  - \( H(\mathbf{T}) = -\sum_{i,j} T_{ij} \log T_{ij} \) 为熵正则化项。
  - \(\lambda\) 为超参数，控制熵项影响（建议设为 0.1，可调）。
- **OT 损失**：
  \[
  \mathcal{L}_{OT}(\mathbf{x}, \mathbf{y}) = \langle \mathbf{T}^*, \mathbf{C} \rangle
  \]
- 将 \(\mathcal{L}_{OT}\) 添加到学生模型的损失函数中，用于反向传播。

---

### 第三步：分块矩阵设计

#### 问题分析
- 若 \( N = M = 512 \)：
  - 成本矩阵 \(\mathbf{C} \in \mathbb{R}^{512 \times 512}\)。
  - FP32 精度下，显存占用：\( 512 \times 512 \times 4 \, \text{bytes} = 1 \, \text{MB} \)。
- 但计算 \(\mathbf{S} = \frac{\mathbf{X} \mathbf{Y}^\top}{\sqrt{V}}\) 需要：
  - \(\mathbf{X} \in \mathbb{R}^{512 \times 151936}\)，FP16 下：\( 512 \times 151936 \times 2 \, \text{bytes} = 148.5 \, \text{MB}\)。
  - \(\mathbf{Y} \in \mathbb{R}^{512 \times 151936}\)，FP16 下：同样 148.5 MB。
  - 中间矩阵 \(\mathbf{S} \in \mathbb{R}^{512 \times 512}\)，FP32 下：1 MB。
- 直接计算 \(\mathbf{X} \mathbf{Y}^\top\) 的矩阵乘法需要约 150 GB 显存（远超 24 GB），因此需要分块。

#### 分块策略
- 将 \(\mathbf{X}\) 和 \(\mathbf{Y}\) 按行分割为 \( K \) 个子块：
  - 每块大小：\(\mathbf{X}_k \in \mathbb{R}^{B \times V}\)，\(\mathbf{Y}_k \in \mathbb{R}^{B \times V}\)，其中 \( B = N / K \)。
  - 选择 \( B = 64 \)（即 \( K = 512 / 64 = 8 \)）。
- 分块计算 \(\mathbf{S}\)：
  - 对于每个子块对 \( (k, l) \)：
    \[
    \mathbf{S}_{k,l} = \frac{\mathbf{X}_k \mathbf{Y}_l^\top}{\sqrt{V}}
    \]
    - \(\mathbf{S}_{k,l} \in \mathbb{R}^{64 \times 64}\)。
    - FP16 矩阵乘法：\( 64 \times 151936 \times 64 \times 2 \, \text{bytes} \approx 1.19 \, \text{GB}\) 中间显存。
  - 总 \(\mathbf{S} = [\mathbf{S}_{k,l}]\) 仍为 \( 512 \times 512 \)。
- 分块计算 \(\mathbf{C}\) 和 \(\mathbf{T}^*\)：
  - 对每个 \(\mathbf{S}_{k,l}\) 应用 softmax 和 \( \mathbf{C}_{k,l} = 1 - \mathbf{S}_{\text{norm}, k,l} \)。
  - 使用 Sinkhorn 算法分块求解 \(\mathbf{T}^*\)。

#### 显存优化
- 只存储当前子块的 \(\mathbf{S}_{k,l}\) 和 \(\mathbf{C}_{k,l}\)，计算完后释放。
- 累加 \(\mathcal{L}_{OT}\) 的贡献，避免存储整个 \(\mathbf{T}^*\)。

---

### 第四步：显存占用分析

#### 模型显存
- **学生模型**（Qwen2.5-1.5B-bnb-4bit）：
  - 4-bit 量化：\( 1.5 \times 10^9 \times 0.5 \, \text{bytes} = 0.75 \, \text{GB} \)。
  - 激活和梯度（FP16）：约 3-4 GB（序列长度 512，batch size 1）。
  - 总计：约 4.5 GB。
- **教师模型**（Qwen2.5-7B，FP16）：
  - 参数：\( 7 \times 10^9 \times 2 \, \text{bytes} = 14 \, \text{GB} \)。
  - 无需梯度，仅前向传播，激活约 2 GB。
  - 总计：约 16 GB。

#### OT 计算显存
- **Logits**：
  - \(\mathbf{X}\) 和 \(\mathbf{Y}\) 各 148.5 MB（FP16）。
- **分块计算**：
  - 每块 \(\mathbf{S}_{k,l}\)：\( 64 \times 64 \times 4 \, \text{bytes} = 16 \, \text{KB} \)。
  - 中间乘法峰值：1.19 GB。
  - 总 OT 计算峰值：约 2 GB（分块后）。

#### 总显存
- 模型：4.5 GB（学生）+ 16 GB（教师）= 20.5 GB。
- OT 计算：2 GB。
- 其他（优化器等）：约 1 GB。
- **总计**：23.5 GB < 24 GB，可行。

---

### 第五步：训练配置

- **Batch Size**：1（受显存限制）。
- **序列长度**：512。
- **优化器**：AdamW（8-bit 优化器进一步节省显存）。
- **超参数**：
  - \(\lambda = 0.1\)（熵正则化）。
  - 学习率：\( 2 \times 10^{-5} \)。

---

### 总结

通过分块矩阵方法，OT 蒸馏可以成功应用于 Qwen2.5-1.5B-bnb-4bit 和 Qwen2.5-7B 的 logits，并在单张 4090 上训练。显存占用控制在 23.5 GB 以内，满足硬件限制。建议在实现时使用 PyTorch 的分块矩阵运算和 Sinkhorn 算法库（如 OTT-JAX）以优化性能。


# 具体实现：
以下是更新后的代码实现，增加了 `padding_id` 和 `target` 参数，以进一步增强灵活性。`padding_id` 用于在计算损失时屏蔽填充 token 的影响，`target` 用于指定目标序列（例如输入的标签或参考输出），从而支持更精确的监督训练。

---

### 更新后的代码实现

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
from bitsandbytes import optim
import numpy as np
from ott.geometry import pointcloud
from ott.problems.linear import linear_problem
from ott.solvers.linear import sinkhorn

# 设置设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 加载模型和分词器
teacher_model_name = "Qwen/Qwen2.5-7B"
student_model_name = "Qwen/Qwen2.5-1.5B-bnb-4bit"
tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)

teacher_model = AutoModelForCausalLM.from_pretrained(teacher_model_name, torch_dtype=torch.float16).to(device)
student_model = AutoModelForCausalLM.from_pretrained(student_model_name, load_in_4bit=True).to(device)

# 设置训练参数
batch_size = 1
seq_length = 512
block_size = 64  # 分块大小
num_blocks = seq_length // block_size  # 8
vocab_size = tokenizer.vocab_size  # 151936
lambda_reg = 0.1  # 熵正则化超参数
temperature = 1.0  # softmax 温度参数
reduction = "mean"  # OT 损失的聚合方式: "mean" 或 "sum"
padding_id = tokenizer.pad_token_id  # 填充 token 的 ID

# 优化器（使用 8-bit AdamW 节省显存）
optimizer = optim.AdamW8bit(student_model.parameters(), lr=2e-5)

# OT 损失计算函数（增加 padding_id 和 target）
def compute_ot_loss(student_logits, teacher_logits, target, block_size, temperature=1.0, reduction="mean", padding_id=None):
    """
    计算分块 OT 损失，加入 temperature、reduction、padding_id 和 target 参数
    Args:
        student_logits: [seq_length, vocab_size]
        teacher_logits: [seq_length, vocab_size]
        target: [seq_length]，目标序列（如标签或输入的下一 token）
        block_size: 分块大小
        temperature: softmax 温度参数
        reduction: 损失聚合方式 ("mean" 或 "sum")
        padding_id: 填充 token 的 ID，用于屏蔽
    Returns:
        ot_loss: OT 损失
    """
    N, V = student_logits.shape  # [512, 151936]
    num_blocks = N // block_size  # 8
    ot_loss = 0.0
    valid_blocks = 0  # 用于统计有效子块数量（排除全填充的情况）

    # 创建掩码以屏蔽 padding
    if padding_id is not None:
        mask = (target != padding_id).float()  # [512]，1 表示有效，0 表示填充
    else:
        mask = torch.ones(N, device=device)  # 默认全有效

    # 分块计算
    for i in range(num_blocks):
        for j in range(num_blocks):
            # 获取子块
            start_i, end_i = i * block_size, (i + 1) * block_size
            start_j, end_j = j * block_size, (j + 1) * block_size
            student_block = student_logits[start_i:end_i]  # [64, 151936]
            teacher_block = teacher_logits[start_j:end_j]  # [64, 151936]
            block_mask = mask[start_i:end_i] * mask[start_j:end_j]  # [64] 或标量

            # 如果子块全为 padding，则跳过
            if padding_id is not None and block_mask.sum() == 0:
                continue

            # 计算相似性矩阵
            S = torch.matmul(student_block, teacher_block.T) / np.sqrt(V)  # [64, 64]
            S = S / temperature  # 应用温度参数
            S_norm = F.softmax(S, dim=-1)  # 行归一化
            C = 1 - S_norm  # 成本矩阵 [64, 64]

            # 使用 OTT 库求解 OT
            geom = pointcloud.PointCloud(student_block, teacher_block, cost_fn=None, scale_cost="mean")
            prob = linear_problem.LinearProblem(geom)
            solver = sinkhorn.Sinkhorn()
            ot_result = solver(prob, epsilon=lambda_reg)
            T_star = ot_result.matrix  # 传输计划 [64, 64]

            # 计算 OT 损失贡献（根据掩码加权）
            block_loss = torch.sum(T_star * C) * block_mask.mean()  # 掩码加权
            ot_loss += block_loss
            valid_blocks += 1

    # 应用 reduction
    if valid_blocks == 0:  # 避免除以零
        return torch.tensor(0.0, device=device)
    if reduction == "mean":
        ot_loss = ot_loss / valid_blocks  # 平均有效子块的损失
    elif reduction == "sum":
        ot_loss = ot_loss  # 直接求和
    else:
        raise ValueError("Reduction must be 'mean' or 'sum'")

    return ot_loss

# 训练循环
def train_step(input_text, target_text=None, temperature=1.0, reduction="mean", padding_id=None):
    # 编码输入和目标
    inputs = tokenizer(input_text, return_tensors="pt", max_length=seq_length, truncation=True, padding="max_length").to(device)
    input_ids = inputs["input_ids"]  # [1, 512]

    if target_text is not None:
        targets = tokenizer(target_text, return_tensors="pt", max_length=seq_length, truncation=True, padding="max_length").to(device)
        target_ids = targets["input_ids"][0]  # [512]
    else:
        target_ids = input_ids[0].clone()  # 默认使用输入作为目标（自回归）

    # 前向传播
    with torch.no_grad():
        teacher_outputs = teacher_model(input_ids, output_hidden_states=False)
        teacher_logits = teacher_outputs.logits[0]  # [512, 151936]

    student_outputs = student_model(input_ids, output_hidden_states=False)
    student_logits = student_outputs.logits[0]  # [512, 151936]

    # 计算 OT 损失
    ot_loss = compute_ot_loss(student_logits, teacher_logits, target_ids, block_size, temperature, reduction, padding_id)

    # 添加交叉熵损失（考虑 padding）
    ce_loss = F.cross_entropy(student_logits.view(-1, vocab_size), target_ids.view(-1), ignore_index=padding_id, reduction=reduction)
    total_loss = ot_loss + ce_loss  # 组合 OT 和 CE 损失

    # 反向传播
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

    return total_loss.item()

# 示例训练
input_text = "这是一个测试输入，用于蒸馏模型。"
target_text = "这是一个测试输出，用于监督训练。"  # 可选的目标文本
for epoch in range(10):  # 假设训练 10 个 epoch
    loss = train_step(input_text, target_text, temperature=temperature, reduction=reduction, padding_id=padding_id)
    print(f"Epoch {epoch + 1}, Loss: {loss:.4f}")

# 保存模型
student_model.save_pretrained("./distilled_qwen_1.5B")
tokenizer.save_pretrained("./distilled_qwen_1.5B")
```

---

### 更新说明

1. **新增超参数**：
   - **`padding_id`**：填充 token 的 ID，默认使用分词器的 `pad_token_id`。
     - 在 OT 损失和交叉熵损失中屏蔽填充 token。
   - **`target`**：目标序列（`target_ids`），用于指定监督训练的标签。
     - 若未提供 `target_text`，默认使用输入 `input_ids`（自回归模式）。

2. **OT 损失函数更新**：
   - **掩码机制**：通过 `mask` 屏蔽填充 token 的影响。
     - 若子块全为 padding，则跳过计算（`continue`）。
     - 对每个子块的损失加权（`block_mask.mean()`），确保只计算有效 token 的贡献。
   - **有效子块计数**：引入 `valid_blocks`，避免全填充序列导致除以零。

3. **训练循环更新**：
   - 支持传入 `target_text`，并将其编码为 `target_ids`。
   - 交叉熵损失使用 `ignore_index=padding_id` 屏蔽填充 token。
   - `train_step` 函数接受 `padding_id` 和 `target_text` 参数。

---

### 使用示例

1. **带目标文本的训练**：
   ```python
   loss = train_step(
       input_text="这是一个测试输入。",
       target_text="这是一个测试输出。",
       temperature=1.0,
       reduction="mean",
       padding_id=padding_id
   )
   ```

2. **自回归模式（无目标文本）**：
   ```python
   loss = train_step(
       input_text="这是一个测试输入。",
       target_text=None,
       temperature=0.5,
       reduction="sum",
       padding_id=padding_id
   )
   ```

3. **调整温度和 reduction**：
   ```python
   loss = train_step(
       input_text="这是一个测试输入。",
       target_text="这是一个测试输出。",
       temperature=2.0,
       reduction="sum",
       padding_id=padding_id
   )
   ```

---

### 显存占用

新增的 `padding_id` 和 `target` 参数对显存影响较小：
- **掩码**：`mask` 为 `[512]` 的浮点张量，仅占用 2 KB。
- **目标序列**：`target_ids` 为 `[512]` 的整数张量，占用约 2 KB。
- 总显存仍约为 23.75 GB，未超出 24 GB 限制。

---

### 注意事项

1. **超参数调优**：
   - `padding_id`：确保与分词器的 `pad_token_id` 一致。
   - `target`：若任务需要特定输出，可提供高质量的目标文本。

2. **性能优化**：
   - 若序列中填充比例较高，可减小 `block_size` 以提高计算效率。
   - 使用混合精度：
     ```python
     from torch.cuda.amp import autocast
     with autocast():
         ot_loss = compute_ot_loss(student_logits, teacher_logits, target_ids, block_size, temperature, reduction, padding_id)
     ```

3. **调试**：
   - 检查掩码效果：
     ```python
     print(f"Valid blocks: {valid_blocks}, OT Loss: {ot_loss.item()}")
     ```

---

### 总结

更新后的代码增加了 `padding_id` 和 `target` 参数，支持屏蔽填充 token 和指定目标序列，使蒸馏过程更适应实际任务需求。显存占用仍控制在 4090 的 24 GB 以内，您可以根据具体数据集调整参数并运行训练。


# prompt:
要求如下：
### 2. 最优传输推理蒸馏（Optimal Transport for Reasoning Distillation）

**目标**：通过序列级OT对齐，解决不同分词器和序列长度的对齐问题，保留上下文语义。

### 关键步骤：

1. **经验分布定义**：
    - 学生和教师的序列分别表示为$\mathbf{x} \in \mathbb{R}^{N \times d}$ 和 $\mathbf{y} \in \mathbb{R}^{M \times D}$，其经验分布为：
    $\mu = \frac{1}{N}\sum_{i=1}^N \delta_{\mathbf{x}i}, \quad \nu = \frac{1}{M}\sum_{j=1}^M \delta_{\mathbf{y}_j}$
    - $N和M$ 分别为学生和教师序列的长度；（即学生序列中每个token获得相同的权重1/N，教师序列中每个token获得相同的权重1/M）
    - d 和 D 是每个 token 的维度；
    - δ(⋅) 用于表示在每个位置的 token；定义每个 token 在序列中的“位置”和分布;
2. **成本矩阵计算**：
    - **相似性矩阵**：通过投影教师嵌入到学生空间：
    $\mathbf{S} = \frac{\mathbf{X} \mathbf{P}(\mathbf{Y})^\top}{\sqrt{d}}$
    其中：
        - $\mathbf{P} \in \mathbb{R}^{D \times d}$为投影矩阵，将教师的嵌入映射到学生的空间，而缩放因子
        
        $\sqrt{d}$确保了数值稳定性
        
        - $X \in \mathbb{R}^{N \times d}$和$Y \in \mathbb{R}^{M \times D}$分别是学生和教师的 token 嵌入表示。
    - **归一化与成本矩阵**：相似度矩阵通过softmax函数按行(row-wise)归一化(这种操作确保了矩阵的每一行在经过归一化后都变成一个概率分布—-符合概率分布性质),确保每一行的和为1；其中，下面的1表示一个全为1的矩阵;
    $\mathbf{S}_{\text{norm}} = \text{softmax}(\mathbf{S}), \quad \mathbf{C} = 1 - \mathbf{S}_{\text{norm}}$
3. **最优传输计划求解**：
    - 熵正则化OT问题：
    - 最优传输问题的目标是寻找一个最优的传输计划$T^*$，以最小化从一个分布到另一个分布的成本
    $\mathbf{T}^* = \arg\min_{\mathbf{T} \in U(\alpha, \beta)} \langle \mathbf{T}, \mathbf{C} \rangle - \frac{1}{\lambda} H(\mathbf{T})$
        - T 是传输矩阵，$T_{ij}$表示从第i个样本到第j个样本的传输量；
        - $C_{ij}$是从第i到第j的传输成本；
        - H(T)是熵正则化项 :$H(T)=−\sum_{i,j}T_{ij}\log T_{ij}$
        - λ 是熵正则化项的超参数，控制熵项对优化的影响；
    - OT损失定义为：
    $\mathcal{L}_{OT}(\mathbf{x}, \mathbf{y}) = \langle \mathbf{T}^*, \mathbf{C} \rangle$
如上展示的是OT进行层级对齐的蒸馏损失介绍，请帮我根据上述介绍的原理，对Qwen-1.5B和Qwen-7B进行蒸馏设计，要求：
1.将蒸馏损失应用于模型最后的logits部分；
2.要求在单张4090上可以进行训练；
3.模型使用：Qwen2.5-1.5B-bnb-4bit和Qwen2.5-7B
4.考虑到logits的维度会导致代价矩阵巨大，请使用分块矩阵的思路进行分割，并分析具体的显存占用情况。

好的，我将用中文重新回答您的问题，并根据您的要求设计基于最优传输（Optimal Transport, OT）的蒸馏方案，将 Qwen2.5-1.5B-bnb-4bit（学生模型）蒸馏自 Qwen2.5-7B（教师模型），将 OT 损失应用于模型的最终 logits 部分，并在单张 NVIDIA 4090 GPU（24 GB 显存）上进行训练。以下是详细设计和显存分析。

---

### 设计概览

1. **目标**：使用 OT 在序列级别对齐学生模型（Qwen2.5-1.5B-bnb-4bit）和教师模型（Qwen2.5-7B）的 logits，保留上下文语义。
2. **应用位置**：将 OT 蒸馏损失应用于模型的最终 logits 层。
3. **硬件限制**：训练需在单张 4090 GPU（24 GB 显存）上运行。
4. **方法**：由于 logits 维度较大导致成本矩阵过大，采用分块矩阵（block matrix）方法分割计算，并分析显存占用。

---

### 第一步：模型设置

- **学生模型**：Qwen2.5-1.5B-bnb-4bit
  - 参数量：约 15 亿。
  - 量化：4-bit（基于 BitsAndBytes），显著降低显存需求。
  - 隐藏层维度（d）：1536（基于 Qwen2.5 架构）。
- **教师模型**：Qwen2.5-7B
  - 参数量：约 70 亿。
  - 默认精度：FP16（为简单起见，假设未量化）。
  - 隐藏层维度（D）：3584（基于 Qwen2.5 架构）。
- **Logits 维度**：词汇表大小 \( V = 151,936 \)（Qwen2.5 分词器），每个 token 的 logits 维度为 \( V \)。

---

### 第二步：基于 Logits 的 OT 蒸馏设计

#### 输入表示
- **学生 Logits**：\(\mathbf{X} \in \mathbb{R}^{N \times V}\)，其中 \( N \) 为序列长度。
- **教师 Logits**：\(\mathbf{Y} \in \mathbb{R}^{M \times V}\)，其中 \( M \) 为序列长度（为简化，假设 \( N = M \) 并设为 512，padding 或截断）。
- **经验分布**：
  - 学生：\(\mu = \frac{1}{N} \sum_{i=1}^N \delta_{\mathbf{x}_i}\)。
  - 教师：\(\nu = \frac{1}{M} \sum_{j=1}^M \delta_{\mathbf{y}_j}\)。

#### 相似性矩阵计算
- 由于学生和教师的 logits 维度相同（均为 \( V \)），无需额外的投影矩阵 \(\mathbf{P}\)，可直接计算：
  \[
  \mathbf{S} = \frac{\mathbf{X} \mathbf{Y}^\top}{\sqrt{V}}
  \]
  - \(\mathbf{S} \in \mathbb{R}^{N \times M}\) 表示相似性矩阵。
  - 缩放因子 \(\sqrt{V}\) 确保数值稳定性。
- **归一化**：按行（row-wise）对 \(\mathbf{S}\) 应用 softmax：
  \[
  \mathbf{S}_{\text{norm}} = \text{softmax}(\mathbf{S})
  \]
  - 每行和为 1，满足概率分布性质。
- **成本矩阵**：
  \[
  \mathbf{C} = 1 - \mathbf{S}_{\text{norm}}
  \]
  - \(\mathbf{C} \in \mathbb{R}^{N \times M}\)。

#### OT 损失计算
- **熵正则化 OT 问题**：
  \[
  \mathbf{T}^* = \arg\min_{\mathbf{T} \in U(\alpha, \beta)} \langle \mathbf{T}, \mathbf{C} \rangle - \frac{1}{\lambda} H(\mathbf{T})
  \]
  - \(\mathbf{T}\) 为传输矩阵，\( T_{ij} \) 表示从学生 token \( i \) 到教师 token \( j \) 的传输量。
  - \( H(\mathbf{T}) = -\sum_{i,j} T_{ij} \log T_{ij} \) 为熵正则化项。
  - \(\lambda\) 为超参数，控制熵项影响（建议设为 0.1，可调）。
- **OT 损失**：
  \[
  \mathcal{L}_{OT}(\mathbf{x}, \mathbf{y}) = \langle \mathbf{T}^*, \mathbf{C} \rangle
  \]
- 将 \(\mathcal{L}_{OT}\) 添加到学生模型的损失函数中，用于反向传播。

---

### 第三步：分块矩阵设计

#### 问题分析
- 若 \( N = M = 512 \)：
  - 成本矩阵 \(\mathbf{C} \in \mathbb{R}^{512 \times 512}\)。
  - FP32 精度下，显存占用：\( 512 \times 512 \times 4 \, \text{bytes} = 1 \, \text{MB} \)。
- 但计算 \(\mathbf{S} = \frac{\mathbf{X} \mathbf{Y}^\top}{\sqrt{V}}\) 需要：
  - \(\mathbf{X} \in \mathbb{R}^{512 \times 151936}\)，FP16 下：\( 512 \times 151936 \times 2 \, \text{bytes} = 148.5 \, \text{MB}\)。
  - \(\mathbf{Y} \in \mathbb{R}^{512 \times 151936}\)，FP16 下：同样 148.5 MB。
  - 中间矩阵 \(\mathbf{S} \in \mathbb{R}^{512 \times 512}\)，FP32 下：1 MB。
- 直接计算 \(\mathbf{X} \mathbf{Y}^\top\) 的矩阵乘法需要约 150 GB 显存（远超 24 GB），因此需要分块。

#### 分块策略
- 将 \(\mathbf{X}\) 和 \(\mathbf{Y}\) 按行分割为 \( K \) 个子块：
  - 每块大小：\(\mathbf{X}_k \in \mathbb{R}^{B \times V}\)，\(\mathbf{Y}_k \in \mathbb{R}^{B \times V}\)，其中 \( B = N / K \)。
  - 选择 \( B = 64 \)（即 \( K = 512 / 64 = 8 \)）。
- 分块计算 \(\mathbf{S}\)：
  - 对于每个子块对 \( (k, l) \)：
    \[
    \mathbf{S}_{k,l} = \frac{\mathbf{X}_k \mathbf{Y}_l^\top}{\sqrt{V}}
    \]
    - \(\mathbf{S}_{k,l} \in \mathbb{R}^{64 \times 64}\)。
    - FP16 矩阵乘法：\( 64 \times 151936 \times 64 \times 2 \, \text{bytes} \approx 1.19 \, \text{GB}\) 中间显存。
  - 总 \(\mathbf{S} = [\mathbf{S}_{k,l}]\) 仍为 \( 512 \times 512 \)。
- 分块计算 \(\mathbf{C}\) 和 \(\mathbf{T}^*\)：
  - 对每个 \(\mathbf{S}_{k,l}\) 应用 softmax 和 \( \mathbf{C}_{k,l} = 1 - \mathbf{S}_{\text{norm}, k,l} \)。
  - 使用 Sinkhorn 算法分块求解 \(\mathbf{T}^*\)。

#### 显存优化
- 只存储当前子块的 \(\mathbf{S}_{k,l}\) 和 \(\mathbf{C}_{k,l}\)，计算完后释放。
- 累加 \(\mathcal{L}_{OT}\) 的贡献，避免存储整个 \(\mathbf{T}^*\)。


请按照上述要求补全下列代码：
class KDTrainer(SFTTrainer):
    def __init__():
        super().__init__(*args, **kwargs)

    def compute_loss():
    
  

# 初始化学生模型
student, _ = FastLanguageModel.from_pretrained(
    model_name=origin_student_path,
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
)

student = FastLanguageModel.get_peft_model(
    student,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
    use_rslora=False,
    loftq_config=None,
)

student.print_trainable_parameters()

# 初始化教师模型
teacher, tokenizer = FastLanguageModel.from_pretrained(
    model_name=teacher_path,
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
)
teacher.eval()

# 定义 Alpaca 格式的 prompt 模板
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token

# 数据集格式化函数
def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs = examples["input"]
    outputs = examples["output"]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
        texts.append(text)
    return {"text": texts}

# 加载并预处理 Alpaca 数据集
dataset = load_dataset("yahma/alpaca-cleaned", split="train[:2000]")
dataset = dataset.map(formatting_prompts_func, batched=True)

val_dataset = load_dataset("yahma/alpaca-cleaned", split="train[2000:3000]")
val_dataset = val_dataset.map(formatting_prompts_func, batched=True)

# 配置训练参数
args = TrainingArguments(

)

# 初始化知识蒸馏训练器
trainer = KDTrainer(
    
 
)
# 开始训练
trainer.train(resume_from_checkpoint=False)
综上，请补全上述代码