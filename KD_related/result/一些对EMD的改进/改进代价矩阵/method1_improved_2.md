在使用余弦相似度构建成本矩阵时，内存占用过大的问题主要源于以下几个方面：

高维词嵌入的计算：学生模型和教师模型的嵌入维度分别为1536和3584，即使通过投影矩阵将教师嵌入投影到学生空间（1536维），计算相似度时仍需处理 (batch_size, seq_len, topk, topk) 的成本矩阵。对于较大的 batch_size、seq_len 或 topk，这会导致内存需求激增。
全对全相似度计算：当前实现中，torch.einsum 计算了所有 topk 个词嵌入之间的余弦相似度，形成一个 (batch_size, seq_len, topk, topk) 的矩阵，这种全连接方式在 topk 较大时（如50或100）会显著增加内存占用。
Sinkhorn算法的中间变量：Sinkhorn 迭代中需要存储 K、u、v 和 T，这些张量与成本矩阵大小相关，进一步加剧了内存压力。
为了在最大化语义相似度的同时减少内存占用，可以从以下几个方面重新设计：


优化方向
1. 减少成本矩阵的维度
当前问题：全对全的 topk × topk 相似度计算导致成本矩阵过大。
优化方案：改为基于“学生-教师对齐”的逐对距离计算，而不是全对全矩阵。
只计算每个学生 topk 词嵌入与对应教师 topk 词嵌入之间的逐对距离，形成一个 (batch_size, seq_len, topk) 的向量，而不是 (topk, topk) 矩阵。
这样可以将内存占用从 O(topk²) 降低到 O(topk)。
语义保留：逐对距离仍然基于投影后的嵌入，能保留语义对齐的核心信息。
2. 使用低秩近似或降维
当前问题：即使投影到1536维，嵌入维度仍然较高，计算成本较大。
优化方案：在投影后进一步对嵌入进行降维（如通过 PCA 或随机投影到更低维度，例如256维或512维），然后再计算距离。
语义保留：低秩近似会损失部分信息，但通过选择合适的降维方法（如基于教师嵌入的 PCA），可以尽量保留主要语义分量。
3. 分块计算
当前问题：一次性计算整个批次的成本矩阵和 Sinkhorn 迭代超出了单张 4090 的显存（24GB）。
优化方案：将 batch_size 或 seq_len 分块处理，逐块计算 OT 损失并累加。
语义保留：分块计算不影响语义一致性，仅改变计算顺序。
4. 替换 Sinkhorn 算法
当前问题：Sinkhorn 算法需要多次迭代并存储中间变量，内存开销大。
优化方案：用更轻量的近似方法（如直接最小化学生和教师概率分布的 KL 散度，或基于 W2 距离的闭式解近似）替代 Sinkhorn。
语义保留：虽然会牺牲部分 OT 的全局最优性，但仍能捕捉语义分布的相似性。