### 2. 最优传输推理蒸馏（Optimal Transport for Reasoning Distillation）

**目标**：通过序列级OT对齐，解决不同分词器和序列长度的对齐问题，保留上下文语义。

### 关键步骤：

1. **经验分布定义**：
    - 学生和教师的序列分别表示为$\mathbf{x} \in \mathbb{R}^{N \times d}$ 和 $\mathbf{y} \in \mathbb{R}^{M \times D}$，其经验分布为：
    $\mu = \frac{1}{N}\sum_{i=1}^N \delta_{\mathbf{x}i}, \quad \nu = \frac{1}{M}\sum_{j=1}^M \delta_{\mathbf{y}_j}$
    - $N和M$ 分别为学生和教师序列的长度；（即学生序列中每个token获得相同的权重1/N，教师序列中每个token获得相同的权重1/M）
    - d 和 D 是每个 token 的维度；
    - δ(⋅) 用于表示在每个位置的 token；定义每个 token 在序列中的“位置”和分布;
2. **成本矩阵计算**：
    - **相似性矩阵**：通过投影教师嵌入到学生空间：
    $\mathbf{S} = \frac{\mathbf{X} \mathbf{P}(\mathbf{Y})^\top}{\sqrt{d}}$
    其中：
        - $\mathbf{P} \in \mathbb{R}^{D \times d}$为投影矩阵，将教师的嵌入映射到学生的空间，而缩放因子
        
        $\sqrt{d}$确保了数值稳定性
        
        - $X \in \mathbb{R}^{N \times d}$和$Y \in \mathbb{R}^{M \times D}$分别是学生和教师的 token 嵌入表示。
    - **归一化与成本矩阵**：相似度矩阵通过softmax函数按行(row-wise)归一化(这种操作确保了矩阵的每一行在经过归一化后都变成一个概率分布—-符合概率分布性质),确保每一行的和为1；其中，下面的1表示一个全为1的矩阵;
    $\mathbf{S}_{\text{norm}} = \text{softmax}(\mathbf{S}), \quad \mathbf{C} = 1 - \mathbf{S}_{\text{norm}}$
3. **最优传输计划求解**：
    - 熵正则化OT问题：
    - 最优传输问题的目标是寻找一个最优的传输计划$T^*$，以最小化从一个分布到另一个分布的成本
    $\mathbf{T}^* = \arg\min_{\mathbf{T} \in U(\alpha, \beta)} \langle \mathbf{T}, \mathbf{C} \rangle - \frac{1}{\lambda} H(\mathbf{T})$
        - T 是传输矩阵，$T_{ij}$表示从第i个样本到第j个样本的传输量；
        - $C_{ij}$是从第i到第j的传输成本；
        - H(T)是熵正则化项 :$H(T)=−\sum_{i,j}T_{ij}\log T_{ij}$
        - λ 是熵正则化项的超参数，控制熵项对优化的影响；
    - OT损失定义为：
    $\mathcal{L}_{OT}(\mathbf{x}, \mathbf{y}) = \langle \mathbf{T}^*, \mathbf{C} \rangle$
如上展示的是OT进行层级对齐的蒸馏损失介绍，请帮我根据上述介绍的原理，对Qwen-1.5B和Qwen-7B进行蒸馏设计，要求：
1.将蒸馏损失应用于模型最后的logits部分；
2.要求在单张4090上可以进行训练；
3.模型使用：Qwen2.5-1.5B-bnb-4bit和Qwen2.5-7B
4.考虑到logits的维度会导致代价矩阵巨大，请使用分块矩阵的思路进行分割，并分析具体的显存占用情况。

好的，我将用中文重新回答您的问题，并根据您的要求设计基于最优传输（Optimal Transport, OT）的蒸馏方案，将 Qwen2.5-1.5B-bnb-4bit（学生模型）蒸馏自 Qwen2.5-7B（教师模型），将 OT 损失应用于模型的最终 logits 部分，并在单张 NVIDIA 4090 GPU（24 GB 显存）上进行训练。以下是详细设计和显存分析。

---

### 设计概览

1. **目标**：使用 OT 在序列级别对齐学生模型（Qwen2.5-1.5B-bnb-4bit）和教师模型（Qwen2.5-7B）的 logits，保留上下文语义。
2. **应用位置**：将 OT 蒸馏损失应用于模型的最终 logits 层。
3. **硬件限制**：训练需在单张 4090 GPU（24 GB 显存）上运行。
4. **方法**：由于 logits 维度较大导致成本矩阵过大，采用分块矩阵（block matrix）方法分割计算，并分析显存占用。

---

### 第一步：模型设置

- **学生模型**：Qwen2.5-1.5B-bnb-4bit
  - 参数量：约 15 亿。
  - 量化：4-bit（基于 BitsAndBytes），显著降低显存需求。
  - 隐藏层维度（d）：1536（基于 Qwen2.5 架构）。
- **教师模型**：Qwen2.5-7B
  - 参数量：约 70 亿。
  - 默认精度：FP16（为简单起见，假设未量化）。
  - 隐藏层维度（D）：3584（基于 Qwen2.5 架构）。
- **Logits 维度**：词汇表大小 \( V = 151,936 \)（Qwen2.5 分词器），每个 token 的 logits 维度为 \( V \)。

---

### 第二步：基于 Logits 的 OT 蒸馏设计

#### 输入表示
- **学生 Logits**：\(\mathbf{X} \in \mathbb{R}^{N \times V}\)，其中 \( N \) 为序列长度。
- **教师 Logits**：\(\mathbf{Y} \in \mathbb{R}^{M \times V}\)，其中 \( M \) 为序列长度（为简化，假设 \( N = M \) 并设为 512，padding 或截断）。
- **经验分布**：
  - 学生：\(\mu = \frac{1}{N} \sum_{i=1}^N \delta_{\mathbf{x}_i}\)。
  - 教师：\(\nu = \frac{1}{M} \sum_{j=1}^M \delta_{\mathbf{y}_j}\)。

#### 相似性矩阵计算
- 由于学生和教师的 logits 维度相同（均为 \( V \)），无需额外的投影矩阵 \(\mathbf{P}\)，可直接计算：
  \[
  \mathbf{S} = \frac{\mathbf{X} \mathbf{Y}^\top}{\sqrt{V}}
  \]
  - \(\mathbf{S} \in \mathbb{R}^{N \times M}\) 表示相似性矩阵。
  - 缩放因子 \(\sqrt{V}\) 确保数值稳定性。
- **归一化**：按行（row-wise）对 \(\mathbf{S}\) 应用 softmax：
  \[
  \mathbf{S}_{\text{norm}} = \text{softmax}(\mathbf{S})
  \]
  - 每行和为 1，满足概率分布性质。
- **成本矩阵**：
  \[
  \mathbf{C} = 1 - \mathbf{S}_{\text{norm}}
  \]
  - \(\mathbf{C} \in \mathbb{R}^{N \times M}\)。

#### OT 损失计算
- **熵正则化 OT 问题**：
  \[
  \mathbf{T}^* = \arg\min_{\mathbf{T} \in U(\alpha, \beta)} \langle \mathbf{T}, \mathbf{C} \rangle - \frac{1}{\lambda} H(\mathbf{T})
  \]
  - \(\mathbf{T}\) 为传输矩阵，\( T_{ij} \) 表示从学生 token \( i \) 到教师 token \( j \) 的传输量。
  - \( H(\mathbf{T}) = -\sum_{i,j} T_{ij} \log T_{ij} \) 为熵正则化项。
  - \(\lambda\) 为超参数，控制熵项影响（建议设为 0.1，可调）。
- **OT 损失**：
  \[
  \mathcal{L}_{OT}(\mathbf{x}, \mathbf{y}) = \langle \mathbf{T}^*, \mathbf{C} \rangle
  \]
- 将 \(\mathcal{L}_{OT}\) 添加到学生模型的损失函数中，用于反向传播。

---

### 第三步：分块矩阵设计

#### 问题分析
- 若 \( N = M = 512 \)：
  - 成本矩阵 \(\mathbf{C} \in \mathbb{R}^{512 \times 512}\)。
  - FP32 精度下，显存占用：\( 512 \times 512 \times 4 \, \text{bytes} = 1 \, \text{MB} \)。
- 但计算 \(\mathbf{S} = \frac{\mathbf{X} \mathbf{Y}^\top}{\sqrt{V}}\) 需要：
  - \(\mathbf{X} \in \mathbb{R}^{512 \times 151936}\)，FP16 下：\( 512 \times 151936 \times 2 \, \text{bytes} = 148.5 \, \text{MB}\)。
  - \(\mathbf{Y} \in \mathbb{R}^{512 \times 151936}\)，FP16 下：同样 148.5 MB。
  - 中间矩阵 \(\mathbf{S} \in \mathbb{R}^{512 \times 512}\)，FP32 下：1 MB。
- 直接计算 \(\mathbf{X} \mathbf{Y}^\top\) 的矩阵乘法需要约 150 GB 显存（远超 24 GB），因此需要分块。

#### 分块策略
- 将 \(\mathbf{X}\) 和 \(\mathbf{Y}\) 按行分割为 \( K \) 个子块：
  - 每块大小：\(\mathbf{X}_k \in \mathbb{R}^{B \times V}\)，\(\mathbf{Y}_k \in \mathbb{R}^{B \times V}\)，其中 \( B = N / K \)。
  - 选择 \( B = 64 \)（即 \( K = 512 / 64 = 8 \)）。
- 分块计算 \(\mathbf{S}\)：
  - 对于每个子块对 \( (k, l) \)：
    \[
    \mathbf{S}_{k,l} = \frac{\mathbf{X}_k \mathbf{Y}_l^\top}{\sqrt{V}}
    \]
    - \(\mathbf{S}_{k,l} \in \mathbb{R}^{64 \times 64}\)。
    - FP16 矩阵乘法：\( 64 \times 151936 \times 64 \times 2 \, \text{bytes} \approx 1.19 \, \text{GB}\) 中间显存。
  - 总 \(\mathbf{S} = [\mathbf{S}_{k,l}]\) 仍为 \( 512 \times 512 \)。
- 分块计算 \(\mathbf{C}\) 和 \(\mathbf{T}^*\)：
  - 对每个 \(\mathbf{S}_{k,l}\) 应用 softmax 和 \( \mathbf{C}_{k,l} = 1 - \mathbf{S}_{\text{norm}, k,l} \)。
  - 使用 Sinkhorn 算法分块求解 \(\mathbf{T}^*\)。

#### 显存优化
- 只存储当前子块的 \(\mathbf{S}_{k,l}\) 和 \(\mathbf{C}_{k,l}\)，计算完后释放。
- 累加 \(\mathcal{L}_{OT}\) 的贡献，避免存储整个 \(\mathbf{T}^*\)。

---

### 第四步：显存占用分析

#### 模型显存
- **学生模型**（Qwen2.5-1.5B-bnb-4bit）：
  - 4-bit 量化：\( 1.5 \times 10^9 \times 0.5 \, \text{bytes} = 0.75 \, \text{GB} \)。
  - 激活和梯度（FP16）：约 3-4 GB（序列长度 512，batch size 1）。
  - 总计：约 4.5 GB。
- **教师模型**（Qwen2.5-7B，FP16）：
  - 参数：\( 7 \times 10^9 \times 2 \, \text{bytes} = 14 \, \text{GB} \)。
  - 无需梯度，仅前向传播，激活约 2 GB。
  - 总计：约 16 GB。

#### OT 计算显存
- **Logits**：
  - \(\mathbf{X}\) 和 \(\mathbf{Y}\) 各 148.5 MB（FP16）。
- **分块计算**：
  - 每块 \(\mathbf{S}_{k,l}\)：\( 64 \times 64 \times 4 \, \text{bytes} = 16 \, \text{KB} \)。
  - 中间乘法峰值：1.19 GB。
  - 总 OT 计算峰值：约 2 GB（分块后）。

#### 总显存
- 模型：4.5 GB（学生）+ 16 GB（教师）= 20.5 GB。
- OT 计算：2 GB。
- 其他（优化器等）：约 1 GB。
- **总计**：23.5 GB < 24 GB，可行。

---

### 第五步：训练配置

- **Batch Size**：1（受显存限制）。
- **序列长度**：512。
- **优化器**：AdamW（8-bit 优化器进一步节省显存）。
- **超参数**：
  - \(\lambda = 0.1\)（熵正则化）。
  - 学习率：\( 2 \times 10^{-5} \)。

---

### 总结

通过分块矩阵方法，OT 蒸馏可以成功应用于 Qwen2.5-1.5B-bnb-4bit 和 Qwen2.5-7B 的 logits，并在单张 4090 上训练。显存占用控制在 23.5 GB 以内，满足硬件限制。建议在实现时使用 PyTorch 的分块矩阵运算和 Sinkhorn 算法库（如 OTT-JAX）以优化性能。


# 具体实现：
下面是更新后的代码实现，在损失函数中增加了 `reduction` 和 `temp`（temperature）等可调节的超参数，以增强灵活性。`reduction` 用于控制 OT 损失的聚合方式（如 `mean` 或 `sum`），`temp` 用于调整 softmax 的温度参数，影响相似性矩阵的平滑程度。同时，我保留了之前的整体结构并加以优化。

---

### 更新后的代码实现

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
from bitsandbytes import optim
import numpy as np
from ott.geometry import pointcloud
from ott.problems.linear import linear_problem
from ott.solvers.linear import sinkhorn

# 设置设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 加载模型和分词器
teacher_model_name = "Qwen/Qwen2.5-7B"
student_model_name = "Qwen/Qwen2.5-1.5B-bnb-4bit"
tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)

teacher_model = AutoModelForCausalLM.from_pretrained(teacher_model_name, torch_dtype=torch.float16).to(device)
student_model = AutoModelForCausalLM.from_pretrained(student_model_name, load_in_4bit=True).to(device)

# 设置训练参数
batch_size = 1
seq_length = 512
block_size = 64  # 分块大小
num_blocks = seq_length // block_size  # 8
vocab_size = tokenizer.vocab_size  # 151936
lambda_reg = 0.1  # 熵正则化超参数
temperature = 1.0  # softmax 温度参数
reduction = "mean"  # OT 损失的聚合方式: "mean" 或 "sum"

# 优化器（使用 8-bit AdamW 节省显存）
optimizer = optim.AdamW8bit(student_model.parameters(), lr=2e-5)

# OT 损失计算函数（增加 temperature 和 reduction）
def compute_ot_loss(student_logits, teacher_logits, block_size, temperature=1.0, reduction="mean"):
    """
    计算分块 OT 损失，加入 temperature 和 reduction 参数
    Args:
        student_logits: [seq_length, vocab_size]
        teacher_logits: [seq_length, vocab_size]
        block_size: 分块大小
        temperature: softmax 温度参数
        reduction: 损失聚合方式 ("mean" 或 "sum")
    Returns:
        ot_loss: OT 损失
    """
    N, V = student_logits.shape  # [512, 151936]
    num_blocks = N // block_size  # 8
    ot_loss = 0.0

    # 分块计算
    for i in range(num_blocks):
        for j in range(num_blocks):
            # 获取子块
            student_block = student_logits[i * block_size:(i + 1) * block_size]  # [64, 151936]
            teacher_block = teacher_logits[j * block_size:(j + 1) * block_size]  # [64, 151936]

            # 计算相似性矩阵
            S = torch.matmul(student_block, teacher_block.T) / np.sqrt(V)  # [64, 64]
            S = S / temperature  # 应用温度参数
            S_norm = F.softmax(S, dim=-1)  # 行归一化
            C = 1 - S_norm  # 成本矩阵 [64, 64]

            # 使用 OTT 库求解 OT
            geom = pointcloud.PointCloud(student_block, teacher_block, cost_fn=None, scale_cost="mean")
            prob = linear_problem.LinearProblem(geom)
            solver = sinkhorn.Sinkhorn()
            ot_result = solver(prob, epsilon=lambda_reg)
            T_star = ot_result.matrix  # 传输计划 [64, 64]

            # 计算 OT 损失贡献
            block_loss = torch.sum(T_star * C)
            ot_loss += block_loss

    # 应用 reduction
    if reduction == "mean":
        ot_loss = ot_loss / (num_blocks ** 2)  # 平均每个子块的损失
    elif reduction == "sum":
        ot_loss = ot_loss  # 直接求和
    else:
        raise ValueError("Reduction must be 'mean' or 'sum'")

    return ot_loss

# 训练循环
def train_step(input_text, temperature=1.0, reduction="mean"):
    # 编码输入
    inputs = tokenizer(input_text, return_tensors="pt", max_length=seq_length, truncation=True, padding="max_length").to(device)
    input_ids = inputs["input_ids"]  # [1, 512]

    # 前向传播
    with torch.no_grad():
        teacher_outputs = teacher_model(input_ids, output_hidden_states=False)
        teacher_logits = teacher_outputs.logits[0]  # [512, 151936]

    student_outputs = student_model(input_ids, output_hidden_states=False)
    student_logits = student_outputs.logits[0]  # [512, 151936]

    # 计算 OT 损失
    ot_loss = compute_ot_loss(student_logits, teacher_logits, block_size, temperature, reduction)

    # 添加交叉熵损失（可选）
    ce_loss = F.cross_entropy(student_logits.view(-1, vocab_size), input_ids.view(-1), reduction=reduction)
    total_loss = ot_loss + ce_loss  # 组合 OT 和 CE 损失

    # 反向传播
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

    return total_loss.item()

# 示例训练
input_text = "这是一个测试输入，用于蒸馏模型。"
for epoch in range(10):  # 假设训练 10 个 epoch
    loss = train_step(input_text, temperature=temperature, reduction=reduction)
    print(f"Epoch {epoch + 1}, Loss: {loss:.4f}")

# 保存模型
student_model.save_pretrained("./distilled_qwen_1.5B")
tokenizer.save_pretrained("./distilled_qwen_1.5B")
```

---

### 更新说明

1. **新增超参数**：
   - **`temperature`**：控制 softmax 的平滑度，默认值为 1.0。
     - 值越大，分布越平滑；值越小，分布越尖锐。
     - 在 `S = S / temperature` 中应用，调整相似性矩阵的敏感度。
   - **`reduction`**：控制 OT 损失的聚合方式。
     - `"mean"`：对所有子块的损失取平均。
     - `"sum"`：直接累加所有子块的损失。
     - 通过 `if-elif` 逻辑实现。

2. **损失函数增强**：
   - OT 损失函数 `compute_ot_loss` 接受 `temperature` 和 `reduction` 参数。
   - 添加了可选的交叉熵损失（CE），与 OT 损失组合，形成 `total_loss`。

3. **训练循环更新**：
   - `train_step` 函数接受 `temperature` 和 `reduction` 参数，便于动态调整。

---

### 使用示例

1. **调整温度**：
   - 平滑分布：`loss = train_step(input_text, temperature=2.0, reduction="mean")`
   - 尖锐分布：`loss = train_step(input_text, temperature=0.5, reduction="mean")`

2. **调整 reduction**：
   - 平均损失：`loss = train_step(input_text, temperature=1.0, reduction="mean")`
   - 总和损失：`loss = train_step(input_text, temperature=1.0, reduction="sum")`

---

### 显存占用

显存占用与之前分析一致，仍控制在 23.75 GB 以内：
- 教师模型：16 GB。
- 学生模型：4.75 GB。
- OT 计算：2 GB（分块后）。
- 其他：1 GB。

新增的 `temperature` 和 `reduction` 不显著增加显存需求，仅影响数值计算。

---

### 注意事项

1. **超参数调优**：
   - `temperature`：建议从 0.5 到 2.0 范围内尝试。
   - `reduction`：若训练数据规模较大，推荐 `"mean"` 以避免损失值过大。

2. **性能优化**：
   - 若需要更低显存，可减小 `block_size` 或启用混合精度：
     ```python
     from torch.cuda.amp import autocast
     with autocast():
         ot_loss = compute_ot_loss(student_logits, teacher_logits, block_size, temperature, reduction)
     ```

3. **扩展性**：
   - 可通过调整 `lambda_reg` 或添加其他损失（如 KL 散度）进一步优化：
     ```python
     kl_loss = F.kl_div(F.log_softmax(student_logits / temperature, dim=-1),
                        F.softmax(teacher_logits / temperature, dim=-1),
                        reduction=reduction)
     total_loss = ot_loss + ce_loss + kl_loss
     ```

---

### 总结

更新后的代码增加了 `temperature` 和 `reduction` 超参数，使 OT 蒸馏更灵活，同时保持在 4090 GPU 的显存限制内。您可以根据具体任务需求调整这些参数，并通过打印损失值观察效果。